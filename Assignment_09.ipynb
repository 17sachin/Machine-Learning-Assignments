{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_09.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPF4rD28QIkT6hbEe3tFnaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Machine-Learning-Assignments/blob/main/Assignment_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is feature engineering, and how does it work? Explain the various aspects of feature\n",
        "engineering in depth.\n",
        "\n",
        "Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised learning. In order to make machine learning work well on new tasks, it might be necessary to design and train better features"
      ],
      "metadata": {
        "id": "9Lab9dUrb4Zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is feature selection, and how does it work? What is the aim of it? What are the various\n",
        "methods of function selection?\n",
        "\n",
        "Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model."
      ],
      "metadata": {
        "id": "coDMWS4qczcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
        "approach?\n",
        "\n",
        "The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it"
      ],
      "metadata": {
        "id": "_waadyk1c5TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Explain the key underlying principle of feature extraction using an example. What are the most\n",
        "widely used function extraction algorithms?\n",
        "\n",
        "Feature extraction involves reducing the number of resources required to describe a large set of data. Feature extraction is a general term for methods of constructing combinations of the variables to get around these problems while still describing the data with sufficient accuracy."
      ],
      "metadata": {
        "id": "qAcA8hMWc-ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Describe the feature engineering process in the sense of a text categorization issue.\n",
        "\n",
        "The most important part of text classification is feature engineering: the process of creating features for a machine learning model from raw text data. In this article, I will explain different methods to analyze text and extract features that can be used to build a classification model"
      ],
      "metadata": {
        "id": "TjlTPMUddGZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uWMh5L0bdNVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Embedded technique\n",
        "\n",
        "Embedded methods are a catch-all group of techniques which perform feature selection as part of the model construction process. The exemplar of this approach is the LASSO method for constructing a linear model, which penalizes the regression coefficients with an L1 penalty, shrinking many of them to zero"
      ],
      "metadata": {
        "id": "kjzz-8-pdNnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Sequential backward exclusion vs. sequential forward selection\n",
        "\n",
        "Sequential floating forward selection (SFFS) starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases. Sequential floating backward selection (SFBS) starts from the full set."
      ],
      "metadata": {
        "id": "DnXTD1cBdWOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.SMC vs. Jaccard coefficient\n",
        "\n",
        "The Jaccard similarity index (sometimes called the Jaccard similarity coefficient) compares members for two sets to see which members are shared and which are distinct. It's a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations"
      ],
      "metadata": {
        "id": "4dKoi5eSddRn"
      }
    }
  ]
}