{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_22.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOENtaX14GIOXHJqQM35wH8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Machine-Learning-Assignments/blob/main/Assignment_22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is polynomial regression and how does it work?\n",
        "\n",
        "Polynomial Regression is a form of Linear regression known as a special case of Multiple linear regression which estimates the relationship as an nth degree polynomial. Polynomial Regression is sensitive to outliers so the presence of one or two outliers can also badly affect the performance."
      ],
      "metadata": {
        "id": "4plw8urMsVMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe how logistic regression works.\n",
        "\n",
        "Logistic regression is a statistical analysis method to predict a binary outcome, such as yes or no, based on prior observations of a data set. ... Based on historical data about earlier outcomes involving the same input criteria, it then scores new cases on their probability of falling into one of two outcome categorie"
      ],
      "metadata": {
        "id": "goHrPzGgsbhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's the difference between hard voting classifiers and soft voting classifiers?\n",
        "\n",
        "Voting Classifier is an estimator that combines models representing different classification algorithms associated with individual weights for confidence. The Voting classifier estimator built by combining different classification models turns out to be stronger meta-classifier that balances out the individual classifiersâ€™ weaknesses on a particular dataset. Voting classifier takes majority voting based on weights applied to the class or class probabilities and assigns a class label to a record based on majority vote. "
      ],
      "metadata": {
        "id": "aHNXOxHKshfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is it possible to distribute a bagging ensemble&#39;s training through several servers to speed up the\n",
        "process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all\n",
        "options.\n",
        "\n",
        "It is quite possible to speed up training of a bagging ensemble, pasting ensembles and Random Forests by distributing it across multiple servers, since each predictor in the ensemble is independent of the others. ... Thus, you have more instances available for training, and your ensemble can perform slightly better."
      ],
      "metadata": {
        "id": "gvivesYissN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the advantage of evaluating out of the bag?\n",
        "\n",
        "Better Predictive Model: OOB_Score helps in the least variance and hence it makes a much better predictive model than a model using other validation techniques. Less Computation: It requires less computation as it allows one to test the data as it is being trained"
      ],
      "metadata": {
        "id": "1FhO5SQHs0Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra\n",
        "randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random\n",
        "Forests?\n",
        "\n",
        "Random Forest chooses the optimum split while Extra Trees chooses it randomly. However, once the split points are selected, the two algorithms choose the best one between all the subset of features. Therefore, Extra Trees adds randomization but still has optimization."
      ],
      "metadata": {
        "id": "CsPPVMQ3s7Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training\n",
        "data?\n",
        "\n",
        "An important hyperparameter for AdaBoost algorithm is the number of decision trees used in the ensemble. Recall that each decision tree used in the ensemble is designed to be a weak learner. That is, it has skill over random prediction, but is not highly skillful."
      ],
      "metadata": {
        "id": "AFcxr2wJs_4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the\n",
        "training set?\n",
        "\n",
        "Choose a relatively high learning rate. \n",
        "\n",
        "Determine the optimum number of trees for this learning rate. \n",
        "\n",
        "Tune tree-specific parameters for decided learning rate and number of trees. \n",
        "\n",
        "Lower the learning rate and increase the estimators proportionally to get more robust models."
      ],
      "metadata": {
        "id": "ZztgCqGztFds"
      }
    }
  ]
}