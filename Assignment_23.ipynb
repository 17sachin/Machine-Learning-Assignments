{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_23.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOsV90OBAN/qTecY1BpxfR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Machine-Learning-Assignments/blob/main/Assignment_23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?\n",
        "\n",
        "Advantages of Dimensionality Reduction\n",
        "It helps in data compression, and hence reduced storage space.\n",
        "It reduces computation time.\n",
        "It also helps remove redundant features, if any.\n",
        "\n",
        "PCA tends to find linear correlations between variables, which is sometimes undesirable. PCA fails in cases where mean and covariance are not enough to define datasets. We may not know how many principal components to keep- in practice, some thumb rules are applied\n",
        "\n"
      ],
      "metadata": {
        "id": "R9M9kTeHNXkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the dimensionality curse?\n",
        "\n",
        "The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions."
      ],
      "metadata": {
        "id": "ecyuxRjHNu5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?\n",
        "\n",
        "No, dimensionality reduction is not reversible in general. It loses information.\n",
        "\n",
        "PCA transformation is not reversible (i.e. getting original data back from Principal component is not possible because some information is lost in the process of dimensionality reduction)"
      ],
      "metadata": {
        "id": "RZQICicEN4Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?\n",
        "\n",
        "PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear because it can at least get rid of useless dimensions. However, if there are no useless dimensions, reducing dimensionality with PCA will lose too much information."
      ],
      "metadata": {
        "id": "wnObnFvuOSXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Assume you are running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?\n",
        "\n",
        "3"
      ],
      "metadata": {
        "id": "uRR_xiqLOeWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n",
        "\n",
        "To handle non-linear datasets, we discussed kernel PCA, which uses kernel methods to project the linearly inseparable data into a higher dimension where it is linearly separable.\n",
        "\n",
        "vanilla PCA-It performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized, with the maximum variance, maximum information is preserved.\n",
        "\n",
        "incremental PCA - Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. It is still dependent on the input data features, but changing the batch size allows for control of memory usage.\n",
        "\n",
        "randomized PCA - \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j5jXr1UYO8j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.How do you assess a dimensionality reduction algorithm's success on your dataset?\n",
        "\n",
        "A dimensionality reduction algorithm is said to work well if it eliminates a significant number of dimensions from the dataset without losing too much information.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FbgrK_FgPvu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Is it logical to use two different dimensionality reduction algorithms in a chain?\n",
        "\n",
        "It can make sense to combine two DR methods.\n"
      ],
      "metadata": {
        "id": "De7QVapUQAiX"
      }
    }
  ]
}